## PNI Ensemble

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_authors.png)

Upgraded PNI.  
SOTA at mvtec_ad:  
99.56% and 98.98% AUROC scores in anomaly detection and localization, respectively.  (PatchCore 99.1%, 98.1%)

### Overview

![PNI ensemble model architecture](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_ensemble.png)



##### Part 1. Modeling Normal Feature Distribution

The anomaly score of the patch-level feature $S(\mathbf{x})$ is estimated as the negative log-likelihood of $p(\Phi_i (\mathbf{x}))$ where $\Phi_i$ is the patch level feature.
$$
S(\mathbf{x}) = - \log p(\Phi_i (\mathbf{x}))
$$
This paper add position and neighboring feature $\Omega$, so the score would be:
$$
S(\mathbf{x}) = - \log p(\Phi_i (\mathbf{x}) | \Omega)
$$
To model the probability from training features, this paper used embedding coreset $C_{emb}$ (the memory bank), so the normal probability of a patch $p(\Phi_i (\mathbf{x})| \Omega)$ can be expresses as:
$$
p(\Phi_i (\mathbf{x})| \Omega) = \sum_{c \in C_{emb}} p(\Phi_i (\mathbf{x}|c, \Omega)) p(c|\Omega)
$$
Since the $p(c|\Omega)$ is a sparse distribution, to simplify:
$$
p(\Phi_i (\mathbf{x})| \Omega) = \mathop{ \max_{c \in C_{emb}}} p(\Phi_i (\mathbf{x}|c, \Omega)) T_\tau(p(c|\Omega))
$$
where $T_\tau(x)$ is defined as: 
$$
T_\tau (x)=\left\{
\begin{array}{rcl}
1 ,       &       x > \tau \\
0,    &      otherwise.\\
\end{array} \right. 
$$
$\tau$ lower than $1 / |C_{emb}|$ guarantees at least one of $c$ in $C_{emb}$ be a normal feature. In this paper, we set $\tau = 1 / (2|C_{emb}|)$ without optimizing.

Approximate $p(c|\Omega )$ :
$$
p(c|\Omega) \approx \frac{p(c|N_p(x)) + p(c|x)}{2}
$$
where $p(c|N_p(x))$ is the normal feature in **neighborhood information** and modeled using **MLP**.

To simplify computation, author use distribution coreset $C_{dist}$ to represent $C_{emb}$ .

The neighborhood information is defined as a set of features that are within a $p \times p$ patch, excluding $\mathbf{x}$ itself: 
$$
N_p(\mathbf{x}) = \{\Phi_i (m, n) | |m-h| \leq p/2, \\
|n-w| \leq p/2, (m,n) \neq (h,w)
\}
$$
To train the MLP, the input is a 1-dimensional vector obtained by concatenating all features in $N_p(\mathbf{x})$ , output has $|C_{dist}|$ nodes. The ground truth used for training is a one-hot vector, where the distribution coreset index closest to the true center feature vector is one, and the cross-entropy loss is calculated with the MLP output.

$p(c|\mathbf{x})$ i.e. $p(c_{dist}| \mathbf{x})$ is the **position information**, the $p(c_{dist}|\mathbf{x})$ is generated by accumulating the indices of $D_{dist}$ for each position $\mathbf{x}$ in all training images using algorithm 1 as follow:

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_histogram.png)

(? 不太懂)

Then $p(\Phi_i(x) | c_{emb})$ is expressed in terms of an exponent of the distance between $\Phi_i(\mathbf{x})$ and $c_{emb}$ as in most existing methods:
$$
p(\Phi_i(x) | c_{emb}, \Omega) \approx p(\Phi_i(\mathbf{x})|x_{emb}) \approx e^{-\lambda ||\Phi_i(x) - c_{emb}||_2}
$$


##### Part 2. Pixel-wise Refinement

Author trained a supervised refinement network using artificial defect image. 

The optimization goal: 
$$
\theta^* = \mathop{argmin_\theta} \sum_{(I, \hat{A}, A) \in D} \ell (f(I, \hat{A}; \theta), A)
$$
where $I$ is an artificially generated anomaly image, and A represents the ground-truth anomaly map of $I$ as a binary map, $\hat{A}$ is an anomaly map estimated from proposed algorithm.

Author used modified DenseNet161 as backbone, and encoder-decoder architecture for $f$. The input is 4-channel RGB image and an anomaly map 

The architecture as followed :

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_refine_network.png)

Loss function:
$$
\ell = (\ell_{reg} + \ell_{grad})/2  \\
\ell_{reg} = \frac{||\hat{A} - A||_2}{HW},  \\ 
\ell_{grad} = \frac{||\bigtriangledown_h \tilde{A} - \bigtriangledown_h A ||_2 
+ ||\bigtriangledown_w \tilde{A} - \bigtriangledown_w A||_2}{2HW}
$$
where the $H$ and $W$ are the width and height of A, $\bigtriangledown_h$ and $\bigtriangledown_w$ are partial derivative operations.

$\ell_{grad}$ improves the refinement results by making the network’s training more concentrated near the edges of the defect region.





FYI, author compared four method to generate synthetic data, CutPaste, CutPaste (scar), DREAM and mannual drawing, the conclusion as quote: 

"As pointed out in CutPaste, training with defects of varying sizes and shapes together prevents the network from optimizing in a naive direction and enables better generalization performance. This is a significant advantage in cases where real abnormal data is unknown. Figure 3 shows the defect image examples generated by 4 methods from normal MVTec AD training data. Defects generated by each method have distinct characteristics. CutPaste creates rectangular defects in larger areas, while CutPaste (scar) produces more detailed and thinner defects. DRÆM and manual methods generate a more complex variety defect patterns. "

Figure 3:

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_synthetic_data_compare.png)



##### Experimental Results

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_results.png)

![](/home/leah/gitlab/karazhan/karazhan/AnomalyDetection.assets/pni_results2.png)